{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition with TensorFlow\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "### About the Dataset\n",
    "A popular component of computer vision and deep learning revolves around identifying faces for various applications from logging into your phone with your face or searching through surveillance images for a particular suspect. This dataset is great for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large quantity of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).\n",
    "\n",
    "https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of this Notebook\n",
    "\n",
    "The goal of this notebook is to create a supervised learning model that can predict a celebrities hair color.  To achieve this goal we have over 100,000 images of celebrities with their hair colors identified.  These images have different backgrounds which may cause some confusion in our model.  However, this will also help the real world application of the model.  We will be using Keras and knn to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic lib imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras as ks\n",
    "from skimage import io\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opening attributes in a new dataframe\n",
    "attr = pd.read_csv('list_attr_celeba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hair columns to categorical values\n",
    "hair = pd.DataFrame(np.where(attr[['Black_Hair','Bald','Blond_Hair','Brown_Hair','Gray_Hair']] > 0, 1, 0),columns=['Black_Hair','Bald','Blond_Hair','Brown_Hair','Gray_Hair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing if all values are 0\n",
    "hair = hair[(hair.T != 0).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting list of target image ids\n",
    "target_images = hair.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing cv2 to help work with images\n",
    "import cv2\n",
    "\n",
    "# Creating a function to format images\n",
    "def load_images(path):\n",
    "    img_data = [] # return the image itself\n",
    "    index = [] # adds an index to reference image\n",
    "    x = -1\n",
    "    for pic in os.listdir(path):\n",
    "        pic_path = os.path.join(path,pic)\n",
    "        for img in os.listdir(pic_path):\n",
    "            x += 1\n",
    "            if x in target_images: # selecting images in target index\n",
    "                img_path = os.path.join(pic_path,img)\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.resize(image, (64, 64))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                img_data.append(image)\n",
    "                index.append(x)\n",
    "    return(np.array(img_data),np.array(index)) # saving image data as array and image number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up fill location for our load_images function\n",
    "train_path = 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running load_images saving images arrays to x and ids to img num\n",
    "(X, img_num) = load_images(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import to split data\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperating data into training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, hair, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86416, 64, 64, 3)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etallen127\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Modeling Tools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init Model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(16, (3, 3), input_shape=(64,64,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3 )))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86416 samples, validate on 42564 samples\n",
      "Epoch 1/11\n",
      "86416/86416 [==============================] - 1927s 22ms/step - loss: 0.5206 - acc: 0.8244 - val_loss: 0.4287 - val_acc: 0.8518\n",
      "Epoch 2/11\n",
      "86416/86416 [==============================] - 1923s 22ms/step - loss: 0.4126 - acc: 0.8655 - val_loss: 0.4238 - val_acc: 0.8816\n",
      "Epoch 3/11\n",
      "86416/86416 [==============================] - 1919s 22ms/step - loss: 0.3829 - acc: 0.8767 - val_loss: 0.5292 - val_acc: 0.8385\n",
      "Epoch 4/11\n",
      "86416/86416 [==============================] - 1922s 22ms/step - loss: 0.3658 - acc: 0.8832 - val_loss: 0.6059 - val_acc: 0.8553\n",
      "Epoch 5/11\n",
      "86416/86416 [==============================] - 1920s 22ms/step - loss: 0.3521 - acc: 0.8883 - val_loss: 0.3787 - val_acc: 0.8708\n",
      "Epoch 6/11\n",
      "86416/86416 [==============================] - 1922s 22ms/step - loss: 0.3416 - acc: 0.8921 - val_loss: 0.4965 - val_acc: 0.8659\n",
      "Epoch 7/11\n",
      "86416/86416 [==============================] - 1917s 22ms/step - loss: 0.3312 - acc: 0.8964 - val_loss: 0.3439 - val_acc: 0.8935\n",
      "Epoch 8/11\n",
      "86416/86416 [==============================] - 1921s 22ms/step - loss: 0.3224 - acc: 0.9009 - val_loss: 0.4223 - val_acc: 0.8791\n",
      "Epoch 9/11\n",
      "86416/86416 [==============================] - 1921s 22ms/step - loss: 0.3120 - acc: 0.9031 - val_loss: 0.3786 - val_acc: 0.8873\n",
      "Epoch 10/11\n",
      "86416/86416 [==============================] - 1925s 22ms/step - loss: 0.3056 - acc: 0.9067 - val_loss: 0.4342 - val_acc: 0.8961\n",
      "Epoch 11/11\n",
      "86416/86416 [==============================] - 1921s 22ms/step - loss: 0.2970 - acc: 0.9099 - val_loss: 0.4221 - val_acc: 0.8855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0059ca1d0>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Model\n",
    "# higher batch and loss 32, .5\n",
    "model.fit(X_train,y_train, epochs=11, batch_size = 32,\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving predictions for new model\n",
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42564, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42564,)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape to fit to classification_report\n",
    "print(y_test.shape)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitting predictions to match y_test shape\n",
    "from keras.utils import np_utils\n",
    "pred = np_utils.to_categorical(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42564, 5)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred fit\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Black Hair       0.97      0.88      0.92     17675\n",
      "       Bald       0.77      0.81      0.79      1403\n",
      " Blond Hair       0.92      0.90      0.91     10216\n",
      " Brown Hair       0.75      0.94      0.83     10897\n",
      "  Gray Hair       0.74      0.86      0.79      2373\n",
      "\n",
      "avg / total       0.88      0.90      0.89     42564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing metrics\n",
    "print(classification_report(pred,y_test,target_names=['Black Hair','Bald','Blond Hair','Brown Hair','Gray Hair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Analysis\n",
    "\n",
    "   Overall we were able to create a pretty successful model averaging about 89% accuracy.  Through trial and error I found consistent hyperparameters for fitting our model.  Before, with a higher batch and/or a lower dropout rate the model had a stronger bias towards the training data.  Being able to reach an accuracy of 93% to 94% but performing much worse on the test data staying in the mid 80s.  After further research, I think this overfitting occurs because the a model with the higher batch size loses some of its ability to generalize as effectively. The lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function on a much larger sample. These minimizers are characterized by large positive eigenvalues.  Similarly I this bias is avoided by increasing the dropout rate because it reduces the number of ‘neurons’ we are gathering information from improving the models ability to generalize.\n",
    "   \n",
    "### Preformance by Color \n",
    "\n",
    "   Looking at the classification report the performance of our model based on hair color isn’t that surprising to me.  Black and blond hair have the highest accuracy which makes since to me because they are the most extreme color values.  While bald only have 1,400 values could easily be confused from other factors like background or skin color.  As for brown hair, I think this would be the hardest, thing like highlights, reflections and lighting would cause a lot of confusion.  In direct sunlight brown hair could easily be mistaken from blond while in a dark photo I could be seen as black.  Last, I thought gray would do better, could be a problem with gray vs bald. But I think it would have benefited the model to have more samples considering it’s the second lowest with 2,300 sample.\n",
    "   \n",
    "### Concerns and Shortcomings\n",
    "\n",
    "   One thing that worries me about this model is the lack of consistency in the number of samples.  Looking at the classification report we can see that the most accurate prediction was for blond hair at 97%.  Which was also the color with the most samples by over 6,000.  Again, the second best accuracy was on the second most common color black.  However brown hair had a similar number of samples and was much worse.  Last, gray and bald had barely any so I think the model lack the proper amount of data to be trained properly.  However, this could easily be solved with more samples, and a balanced number of samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
